{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hassan\\Documents\\Projects\\LLMs\\RAG-Projects\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['USER_AGENT'] = 'chrome'\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b baseline\n",
    "#dataset = pd.read_csv('eval_set_03-11-2024_08-31-29.csv', index_col=0)\n",
    "# 1b baseline\n",
    "dataset = pd.read_csv('eval_set_07-11-2024_16-24-25.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>retrieved_context</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is prompt engineering?</td>\n",
       "      <td>Prompt engineering is the process of designing...</td>\n",
       "      <td>OpenAI Cookbook has many in-depth examples for...</td>\n",
       "      <td>Prompt Engineering, also known as In-Context P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the basic approaches for prompting a ...</td>\n",
       "      <td>The basic approaches for prompting a language ...</td>\n",
       "      <td>OpenAI Cookbook has many in-depth examples for...</td>\n",
       "      <td>Zero-shot and few-shot learning are two of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the issues in few-shot learning that ...</td>\n",
       "      <td>The issues in few-shot learning that lead to p...</td>\n",
       "      <td>Zero-Shot#\\nZero-shot learning is to simply fe...</td>\n",
       "      <td>(1) Majority label bias exists if the distribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Chain-of-Thought (CoT) prompting?</td>\n",
       "      <td>Chain-of-Thought (CoT) prompting is a techniqu...</td>\n",
       "      <td>Definition: Determine which category the quest...</td>\n",
       "      <td>Chain-of-thought (CoT) prompting generates a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the types of Chain-of-Thought prompts?</td>\n",
       "      <td>The question asks for the category \"Quantity\" ...</td>\n",
       "      <td>References#\\n[1] Zhao et al. “Calibrate Before...</td>\n",
       "      <td>Two main types of CoT prompting:\\n\\nFew-shot C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                        What is prompt engineering?   \n",
       "1  What are the basic approaches for prompting a ...   \n",
       "2  What are the issues in few-shot learning that ...   \n",
       "3          What is Chain-of-Thought (CoT) prompting?   \n",
       "4    What are the types of Chain-of-Thought prompts?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Prompt engineering is the process of designing...   \n",
       "1  The basic approaches for prompting a language ...   \n",
       "2  The issues in few-shot learning that lead to p...   \n",
       "3  Chain-of-Thought (CoT) prompting is a techniqu...   \n",
       "4  The question asks for the category \"Quantity\" ...   \n",
       "\n",
       "                                   retrieved_context  \\\n",
       "0  OpenAI Cookbook has many in-depth examples for...   \n",
       "1  OpenAI Cookbook has many in-depth examples for...   \n",
       "2  Zero-Shot#\\nZero-shot learning is to simply fe...   \n",
       "3  Definition: Determine which category the quest...   \n",
       "4  References#\\n[1] Zhao et al. “Calibrate Before...   \n",
       "\n",
       "                                           reference  \n",
       "0  Prompt Engineering, also known as In-Context P...  \n",
       "1  Zero-shot and few-shot learning are two of the...  \n",
       "2  (1) Majority label bias exists if the distribu...  \n",
       "3  Chain-of-thought (CoT) prompting generates a s...  \n",
       "4  Two main types of CoT prompting:\\n\\nFew-shot C...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (LLM-as-a-Judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up llama 3.2:3b as the judge model\n",
    "judge_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\", \n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "relevance_prompt = PromptTemplate(\n",
    "    template=\"\"\"Evaluate the relevance score of the provided context to the question shared here.\n",
    "    \n",
    "    Context: {documents}\n",
    "    Question: {question}\n",
    "    \n",
    "    To calculate the relevance score. Do the following step by step. Break the provided context into\n",
    "    distinct statements. Find out how many of these statements provide relevant information to answering \n",
    "    the question. Then, calculate the relevance score as the no. of total statements that are relevant divided by the total\n",
    "    no. of statements in the context.\n",
    "\n",
    "    Provide the response strictly in the following JSON format:\n",
    "\n",
    "\n",
    "    \"{{\n",
    "      \"relevance_score\": <relevance score>,\n",
    "      \"total_relevant_statements\": <total no. of relevant statements in context>,\n",
    "      \"total_statements_in_context\": <total no. of statements in context>,\n",
    "      # \"relevant_statements\": ['statement 1', 'statement 2', ...],\n",
    "      # \"statements_in_context\": ['statement 1', 'statement 2', ...]\n",
    "\n",
    "    }}\"\n",
    "\n",
    "    \n",
    "    Confirm that the JSON format is correct with no extra or missing brackets.\n",
    "    Confirm that the calculation for relevance score is correct and Respond only \n",
    "    with the JSON output without any additional commentary.\n",
    "    \"\"\",\n",
    "    input_variables=[\"documents\",\"question\"],\n",
    ")\n",
    "\n",
    "recall_prompt = PromptTemplate(\n",
    "    template=\"\"\"Evaluate the recall score of the provided reference to the context shared here.\n",
    "    \n",
    "    Reference: {reference}\n",
    "    Context: {documents}\n",
    "\n",
    "    To calculate the recall score. Do the following step by step. Break the provided reference into\n",
    "    distinct statements.Find out how many of \n",
    "    these statements in reference can be attributed to the information provided in the context. Then, calculate \n",
    "    the recall score as the no. of total statements that can be attributed to context divided by the total \n",
    "    no. of statements in the reference.\n",
    "\n",
    "    Provide the response strictly in the following JSON format:\n",
    "\n",
    "\n",
    "    \"{{\n",
    "      \"recall_score\": <recall score>,\n",
    "      \"total_attributed_statements\": <total no. of attributed statements in reference>,\n",
    "      \"total_statements_in_reference\": <total no. of statements in context>,\n",
    "      # \"attributed_statements\": <text of attributed statements as a list>,\n",
    "      # \"statements_in_reference\": <text of all statements in reference as a list>\n",
    "\n",
    "    }}\"\n",
    "\n",
    "    \n",
    "    Confirm that the JSON format is correct with no extra or missing brackets.\n",
    "    Confirm that the calculation for recall score is correct and Respond only \n",
    "    with the JSON output without any additional commentary.\n",
    "    \"\"\",\n",
    "    input_variables=[\"reference\",\"documents\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "faithfulness_prompt = PromptTemplate(\n",
    "    template=\"\"\"Evaluate the faithfulness of the generated answer to the question based on the provided documents.\n",
    "    Faithfulness is calculated as the number of claims in the answer that can be inferred from the documents divided\n",
    "    by the total no. of claims in the answer.\n",
    "    \n",
    "    Provide the response in the following JSON format:\n",
    "     \n",
    "    \"{{\n",
    "      \"faithfulness_score\": <score>,\n",
    "      \"total_inferred_claims\": <total inferred claims in generated answer>,\n",
    "      \"total_claims\": <total claims in generated answer>,\n",
    "      # \"inferred_claims\": [\"claim 1\", \"claim 2\", ...],\n",
    "      # \"claims_in_answer\": [\"claim 1\", \"claim 2\", ...]\n",
    "    }}\"\n",
    "\n",
    "    Question: {question}\n",
    "    Documents: {documents}\n",
    "    Generated Answer: {answer}\n",
    "    \n",
    "    Confirm that the JSON format is correct with no extra or missing brackets. Respond only with the JSON output\n",
    "    without any additional commentary.\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\", \"answer\"],\n",
    ")\n",
    "\n",
    "response_quality_prompt = PromptTemplate(\n",
    "    template=\"\"\"Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant for the question below. \n",
    "    Consider the correctness, relevance, and completeness of the assistant's answer compared to the reference answer.\n",
    "    \n",
    "    If the assistant's answer matches the quality of the reference answer in all respects, respond with \"10\" only.\n",
    "    If the answer is partially correct or lacks completeness or relevance, assign an appropriate score between 1 and 9\n",
    "    and respond with this score only.\n",
    "    If the assistant's answer is incorrect or irrelevant, respond with \"0\" only. \n",
    "    Do not respond with additional commentary in your response.\n",
    "    \n",
    "    Question: {question}\n",
    "    Reference Answer: {reference}\n",
    "    Assistant's Answer: {answer}\n",
    "    \n",
    "    Score:\"\"\",\n",
    "    input_variables=[\"question\", \"reference\", \"answer\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator class for calculating various evaluation metrics for the judge model\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, judge_llm):\n",
    "        self.judge_llm = judge_llm\n",
    "    \n",
    "    def evaluate_relevance(self, documents, question):\n",
    "\n",
    "        relevance_score = self.judge_llm.invoke(relevance_prompt.format(documents=documents, question=question))\n",
    "        return relevance_score\n",
    "    \n",
    "    def evaluate_recall(self, reference, documents):\n",
    "\n",
    "        relevance_score = self.judge_llm.invoke(recall_prompt.format(reference=reference, documents=documents))\n",
    "        return relevance_score\n",
    "\n",
    "    def evaluate_faithfulness(self, question, documents, answer):\n",
    "\n",
    "        faithfulness_score = self.judge_llm.invoke(faithfulness_prompt.format(question=question, documents=documents, answer=answer))\n",
    "        return faithfulness_score\n",
    "\n",
    "\n",
    "    def evaluate_quality(self, question, answer, reference):\n",
    "\n",
    "        quality_score = self.judge_llm.invoke(response_quality_prompt.format(question=question, reference=reference, answer=answer))\n",
    "        return quality_score\n",
    "    \n",
    "    \n",
    "# Initialize evaluator\n",
    "rag_evaluator = RAGEvaluator(judge_llm=judge_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_faithfulness_score(response):\n",
    "    \"\"\"\n",
    "    Extracts the JSON output from the LLM's response content.\n",
    "\n",
    "    Parameters:\n",
    "    response (AIMessage or str): The response from the LLM, containing a JSON-formatted faithfulness score.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the parsed JSON output, or None if parsing fails.\n",
    "    \"\"\"\n",
    "    # Ensure response is a string\n",
    "    response_content = response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "    # Regular expression to find JSON block in the response\n",
    "    json_match = re.search(r'\\{[\\s\\S]*\\}', response_content)\n",
    "    \n",
    "    if json_match:\n",
    "        json_str = json_match.group()  # Extract JSON string\n",
    "        try:\n",
    "            # Parse JSON string to Python dictionary\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to decode JSON.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No JSON content found in the response.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_relevance_score(response):\n",
    "    # Ensure response is a string\n",
    "    response_content = response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "    # Regular expression to find JSON block in the response\n",
    "    json_match = re.search(r'\\{[\\s\\S]*\\}', response_content)\n",
    "    \n",
    "    if json_match:\n",
    "        json_str = json_match.group()  # Extract JSON string\n",
    "        try:\n",
    "            # Parse JSON string to Python dictionary\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to decode JSON.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No JSON content found in the response.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_recall_score(response):\n",
    "    # Ensure response is a string\n",
    "    response_content = response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "    # Regular expression to find JSON block in the response\n",
    "    json_match = re.search(r'\\{[\\s\\S]*\\}', response_content)\n",
    "    \n",
    "    if json_match:\n",
    "        json_str = json_match.group()  # Extract JSON string\n",
    "        try:\n",
    "            # Parse JSON string to Python dictionary\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to decode JSON. Please check the response format.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No JSON content found in the response.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response_quality_score(response):\n",
    "    response = StrOutputParser().parse(response).content\n",
    "    # Use regex to find the binary relevance score (either 0 or 1)\n",
    "    match = re.search(r\"\\b(10|[0-9])\\b\", response)\n",
    "    return int(match.group()) if match else None  # Return score as integer, or None if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_relevance_scores = []\n",
    "# for _, row in dataset.iterrows():\n",
    "#     response = rag_evaluator.evaluate_binary_relevance(row['question'], row['retrieved_context'])\n",
    "#     parsed_response = extract_binary_relevance_score(response)\n",
    "#     binary_relevance_scores.append(parsed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_scores = []\n",
    "relevance_responses = [] \n",
    "for _, row in dataset.iterrows():\n",
    "    response = rag_evaluator.evaluate_relevance(row['retrieved_context'], row['question'])\n",
    "    parsed_response = extract_relevance_score(response)\n",
    "    relevance_scores.append(parsed_response.get(\"relevance_score\"))\n",
    "    relevance_responses.append(parsed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\"relevance_score\": 0.8, \"total_relevant_statements\": 4, \"total_statements_in_context\": 5}', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2024-11-07T12:29:37.5278089Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 27540304900, 'load_duration': 65772200, 'prompt_eval_count': 1420, 'prompt_eval_duration': 23227442000, 'eval_count': 31, 'eval_duration': 4242957000}, id='run-96a33431-e54e-4553-bb1a-7a63ff7fa074-0', usage_metadata={'input_tokens': 1420, 'output_tokens': 31, 'total_tokens': 1451})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5,\n",
       " 0.8,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 4,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.67,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 0.5,\n",
       " 0.4,\n",
       " 0.2,\n",
       " 0.8,\n",
       " 0.8]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores = []\n",
    "recall_responses = [] \n",
    "for _, row in dataset.iterrows():\n",
    "    response = rag_evaluator.evaluate_recall(row['reference'], row['retrieved_context'])\n",
    "    parsed_response = extract_recall_score(response)\n",
    "    recall_scores.append(parsed_response.get(\"recall_score\"))\n",
    "    recall_responses.append(parsed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_scores = []\n",
    "faithfulness_responses = []\n",
    "for _, row in dataset.iterrows():\n",
    "    response = rag_evaluator.evaluate_faithfulness(row['question'],row['retrieved_context'], row['answer'])\n",
    "    parsed_response = extract_faithfulness_score(response)\n",
    "    faithfulness_scores.append(parsed_response.get(\"faithfulness_score\"))\n",
    "    faithfulness_responses.append(parsed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.5, 0.75, 1, 1, 0.5, 0.5, 0.75, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1, 0.5, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_quality_scores = []\n",
    "for _, row in dataset.iterrows():\n",
    "    response = rag_evaluator.evaluate_quality(row['question'], row['answer'], row['reference'])\n",
    "    parsed_response = extract_response_quality_score(response)\n",
    "    response_quality_scores.append(parsed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 8, 6, 8, 6, 6, 6, 8, 8, 8, 6, 6, 9, 6, 8, 8, 6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_quality_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['relevance_score'] = relevance_scores\n",
    "dataset['relevance_response'] = relevance_responses\n",
    "dataset['recall_score'] = recall_scores\n",
    "dataset['recall_response'] = recall_responses\n",
    "dataset['faithfulness_score'] = faithfulness_scores\n",
    "dataset['faithfulness_response'] = faithfulness_responses\n",
    "dataset['response_quality_score'] = response_quality_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>retrieved_context</th>\n",
       "      <th>reference</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>relevance_response</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>recall_response</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>faithfulness_response</th>\n",
       "      <th>response_quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is prompt engineering?</td>\n",
       "      <td>Prompt engineering is the process of designing...</td>\n",
       "      <td>OpenAI Cookbook has many in-depth examples for...</td>\n",
       "      <td>Prompt Engineering, also known as In-Context P...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'relevance_score': 0.5, 'total_relevant_state...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'recall_score': 0.5, 'total_attributed_statem...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>{'faithfulness_score': 0.5, 'total_inferred_cl...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the basic approaches for prompting a ...</td>\n",
       "      <td>The basic approaches for prompting a language ...</td>\n",
       "      <td>OpenAI Cookbook has many in-depth examples for...</td>\n",
       "      <td>Zero-shot and few-shot learning are two of the...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'relevance_score': 0.8, 'total_relevant_state...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'recall_score': 0.5, 'total_attributed_statem...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>{'faithfulness_score': 0.5, 'total_inferred_cl...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the issues in few-shot learning that ...</td>\n",
       "      <td>The issues in few-shot learning that lead to p...</td>\n",
       "      <td>Zero-Shot#\\nZero-shot learning is to simply fe...</td>\n",
       "      <td>(1) Majority label bias exists if the distribu...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'relevance_score': 0.5, 'total_relevant_state...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'recall_score': 0.5, 'total_attributed_statem...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'faithfulness_score': 0.75, 'total_inferred_c...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Chain-of-Thought (CoT) prompting?</td>\n",
       "      <td>Chain-of-Thought (CoT) prompting is a techniqu...</td>\n",
       "      <td>Definition: Determine which category the quest...</td>\n",
       "      <td>Chain-of-thought (CoT) prompting generates a s...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'relevance_score': 0.5, 'total_relevant_state...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'recall_score': 0.5, 'total_attributed_statem...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>{'faithfulness_score': 1, 'total_inferred_clai...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the types of Chain-of-Thought prompts?</td>\n",
       "      <td>The question asks for the category \"Quantity\" ...</td>\n",
       "      <td>References#\\n[1] Zhao et al. “Calibrate Before...</td>\n",
       "      <td>Two main types of CoT prompting:\\n\\nFew-shot C...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{'relevance_score': 4, 'total_relevant_stateme...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'recall_score': 0.5, 'total_attributed_statem...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>{'faithfulness_score': 1, 'total_inferred_clai...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                        What is prompt engineering?   \n",
       "1  What are the basic approaches for prompting a ...   \n",
       "2  What are the issues in few-shot learning that ...   \n",
       "3          What is Chain-of-Thought (CoT) prompting?   \n",
       "4    What are the types of Chain-of-Thought prompts?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Prompt engineering is the process of designing...   \n",
       "1  The basic approaches for prompting a language ...   \n",
       "2  The issues in few-shot learning that lead to p...   \n",
       "3  Chain-of-Thought (CoT) prompting is a techniqu...   \n",
       "4  The question asks for the category \"Quantity\" ...   \n",
       "\n",
       "                                   retrieved_context  \\\n",
       "0  OpenAI Cookbook has many in-depth examples for...   \n",
       "1  OpenAI Cookbook has many in-depth examples for...   \n",
       "2  Zero-Shot#\\nZero-shot learning is to simply fe...   \n",
       "3  Definition: Determine which category the quest...   \n",
       "4  References#\\n[1] Zhao et al. “Calibrate Before...   \n",
       "\n",
       "                                           reference  relevance_score  \\\n",
       "0  Prompt Engineering, also known as In-Context P...              0.5   \n",
       "1  Zero-shot and few-shot learning are two of the...              0.8   \n",
       "2  (1) Majority label bias exists if the distribu...              0.5   \n",
       "3  Chain-of-thought (CoT) prompting generates a s...              0.5   \n",
       "4  Two main types of CoT prompting:\\n\\nFew-shot C...              4.0   \n",
       "\n",
       "                                  relevance_response  recall_score  \\\n",
       "0  {'relevance_score': 0.5, 'total_relevant_state...           0.5   \n",
       "1  {'relevance_score': 0.8, 'total_relevant_state...           0.5   \n",
       "2  {'relevance_score': 0.5, 'total_relevant_state...           0.5   \n",
       "3  {'relevance_score': 0.5, 'total_relevant_state...           0.5   \n",
       "4  {'relevance_score': 4, 'total_relevant_stateme...           0.5   \n",
       "\n",
       "                                     recall_response  faithfulness_score  \\\n",
       "0  {'recall_score': 0.5, 'total_attributed_statem...                0.50   \n",
       "1  {'recall_score': 0.5, 'total_attributed_statem...                0.50   \n",
       "2  {'recall_score': 0.5, 'total_attributed_statem...                0.75   \n",
       "3  {'recall_score': 0.5, 'total_attributed_statem...                1.00   \n",
       "4  {'recall_score': 0.5, 'total_attributed_statem...                1.00   \n",
       "\n",
       "                               faithfulness_response  response_quality_score  \n",
       "0  {'faithfulness_score': 0.5, 'total_inferred_cl...                       8  \n",
       "1  {'faithfulness_score': 0.5, 'total_inferred_cl...                       8  \n",
       "2  {'faithfulness_score': 0.75, 'total_inferred_c...                       6  \n",
       "3  {'faithfulness_score': 1, 'total_inferred_clai...                       8  \n",
       "4  {'faithfulness_score': 1, 'total_inferred_clai...                       6  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"RAG_evaluation_{timestamp}.csv\"\n",
    "dataset.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>response_quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    relevance_score  recall_score  faithfulness_score  response_quality_score\n",
       "0              0.50           0.5                0.50                       8\n",
       "1              0.80           0.5                0.50                       8\n",
       "2              0.50           0.5                0.75                       6\n",
       "3              0.50           0.5                1.00                       8\n",
       "4              4.00           0.5                1.00                       6\n",
       "5              0.80           0.5                0.50                       6\n",
       "6              0.80           0.5                0.50                       6\n",
       "7              0.80           0.5                0.75                       8\n",
       "8              0.67           0.5                0.50                       8\n",
       "9              0.50           0.5                0.50                       8\n",
       "10             0.50           0.5                0.50                       6\n",
       "11             0.80           0.5                0.50                       6\n",
       "12             0.50           0.5                0.50                       9\n",
       "13             0.40           0.5                0.50                       6\n",
       "14             0.20           0.5                1.00                       8\n",
       "15             0.80           0.5                0.50                       8\n",
       "16             0.80           0.5                1.00                       6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = dataset[['relevance_score','recall_score','faithfulness_score','response_quality_score']]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_relevance_score</th>\n",
       "      <th>mean_recall_score</th>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <th>mean_response_quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815882</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>7.117647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_relevance_score  mean_recall_score  mean_faithfulness_score  \\\n",
       "0              0.815882                0.5                 0.647059   \n",
       "\n",
       "   mean_response_quality_score  \n",
       "0                     7.117647  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores = pd.DataFrame(scores.mean()).transpose()\n",
    "mean_scores.columns = ['mean_'+col for col in mean_scores.columns]\n",
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
