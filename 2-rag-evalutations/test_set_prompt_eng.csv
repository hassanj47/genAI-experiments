Query,Answer
What is prompt engineering?,"Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science, and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics."
What are the basic approaches for prompting a language model?,"Zero-shot and few-shot learning are two of the most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance."
What are the issues in few-shot learning that lead to performance variance?,(1) Majority label bias exists if the distribution of labels among the examples is unbalanced; (2) Recency bias refers to the tendency where the model may repeat the label at the end; (3) Common token bias indicates that LLM tends to produce common tokens more often than rare tokens.
What is Chain-of-Thought (CoT) prompting?,"Chain-of-thought (CoT) prompting generates a sequence of short sentences to describe reasoning logics step by step, known as reasoning chains or rationales, to eventually lead to the final answer. The benefit of CoT is more pronounced for complicated reasoning tasks, while using large models (e.g., with more than 50B parameters)."
What are the types of Chain-of-Thought prompts?,"Two main types of CoT prompting:

Few-shot CoT: Prompting the model with a few demonstrations, each containing manually written or model-generated reasoning chains.
Zero-shot CoT: Using natural language statements like 'Let's think step by step' to explicitly encourage the model to generate reasoning chains before answering."
What are some tips for example selection in few-shot learning?,"Choose examples that are semantically similar to the test example using NN clustering in the embedding space. A graph-based approach can also be used, where samples are selected based on their embedding cosine similarity, encouraging the selection of diverse samples."
What is self-consistency sampling?,Self-consistency sampling is to sample multiple outputs with temperature > 0 and then select the best one out of these candidates. The general solution is to pick the majority vote.
What are some methods for automatic prompt design?,"Methods like AutoPrompt, Prefix-Tuning, P-Tuning, and Prompt-Tuning treat prompts as trainable parameters and optimize them directly on the embedding space via gradient descent. Another method, Automatic Prompt Engineer (APE), searches over a pool of model-generated instruction candidates and filters them using a score function."
What are some factors that influence example ordering in few-shot prompting?,"Keep the selection of examples diverse and relevant to the test sample, and in random order to avoid majority label bias and recency bias. Increasing model sizes or including more training examples does not reduce variance among different permutations of in-context examples."
How does Tree of Thoughts (ToT) extend Chain of Thought prompting?,"Tree of Thoughts (ToT) extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. The search process can be BFS or DFS, while each state is evaluated by a classifier or majority vote."
What is the role of instruction prompting in language models?,"Instruction prompting involves directly giving instructions to the model to describe the task requirement in detail. This approach is used to reduce token consumption, improve alignment with human intent, and reduce the cost of communication."
"What is the InstructGPT model, and how does it differ from traditional models?","InstructGPT finetunes a pretrained model with high-quality tuples of (task instruction, input, ground truth output) to improve alignment with user intention and follow instructions more accurately. RLHF (Reinforcement Learning from Human Feedback) is commonly used for this fine-tuning."
How does Chain-of-Thought (CoT) prompting benefit reasoning tasks?,"The benefit of CoT prompting is more pronounced for complicated reasoning tasks, especially with large models that have more than 50B parameters. It helps break down the reasoning process step by step."
How does Q-learning contribute to few-shot learning sample selection?,"Q-learning has been explored for sample selection in few-shot learning by some researchers, as mentioned by Zhang et al. (2022). It helps in identifying examples that could improve model performance based on reinforcement learning principles."
"What is the purpose of using the ""Let's think step by step"" prompt in CoT prompting?","Using the prompt 'Let's think step by step' explicitly encourages the model to generate reasoning chains first, which helps it to break down a problem into manageable steps before arriving at the final answer."
"What is Self-Ask, and how does it work?",Self-Ask is a method where the model is prompted to ask follow-up questions iteratively to construct a thought process. These questions can then be answered using external tools like search engines or APIs to guide the model towards a more accurate response.
How does the Self-Ask method integrate with external tools?,"Self-Ask can be combined with methods like IRCoT (Interleaving Retrieval CoT) and ReAct (Reason + Act), where the model uses external tools like Wikipedia APIs to retrieve relevant information and add it back into the context for reasoning."
